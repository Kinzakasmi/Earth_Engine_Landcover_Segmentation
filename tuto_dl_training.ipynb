{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to this tutorial ! \n",
    "            \n",
    "## There are 6 different steps :   \n",
    ">- 1) Import libraries and define parameters.  \n",
    ">    *Variables followed by **#@param** are variables, you can change them.*\n",
    ">- 2) Load and process your dataset.\n",
    ">- 3) Set/find the loss, metrics and optimal hyper-parameters.\n",
    ">- 4) Train and evaluate your model.  \n",
    ">    *If you want to load a pretrained model, you can skip these steps and go to the next section.*\n",
    ">- 5) Predict your image.\n",
    ">- 6) Visualise your predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Libraries\n",
    "### First create a new **virtual environment** then install all requirements by running the following :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "source": [
    "### Add all folders you will need"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_folders\n",
    "create_folders()"
   ]
  },
  {
   "source": [
    "## Your directory shoud be as following :\n",
    "Check if the folders (the ones **in bold**) are in your directory.\n",
    "- **Main folder**\n",
    "    >- **models**\n",
    "    >    >* .joblib files (sklearn models)\n",
    "    >    >* .sav files (mappers such as pca and umap)\n",
    "    >    >* folders (tensorflow models)\n",
    "    >- **results**\n",
    "    >    >* .png images (confusion matrices)\n",
    "    >    >* .log files (tensorflow training curves)\n",
    "    >- **data**\n",
    "    >    >- **train**\n",
    "    >    >    * train*.tfrecord.gz files (training dataset)\n",
    "    >    >- **eval**\n",
    "    >    >    * traineval*.tfrecord.gz files (evaluation dataset)\n",
    "    >    >- **inference**\n",
    "    >    >   * .tfrecord.gz files (inference dataset)\n",
    "    >    >   * *-mixer.json files (needed for georeferencing, if you want to add the prediction to Earth Engine Editor)\n",
    "    >    >- **predictions**\n",
    "    >    >    - **colored_pipes**\n",
    "    >    >        * .kml files (colored-pipe nets corresponding to labels)\n",
    "    >    >    - **kml**\n",
    "    >    >        * .kml files and corresponding .png images (mask-prediction images)\n",
    "    >    >    - **tfrecords**\n",
    "    >    >        * .TFRecord files (needed if you want to add the prediction to Earth Engine Editor)\n",
    "    >    >    * .csv files"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Import, authenticate and initialize the Earth Engine library. If you have a gmail account, do so with yours, if not, you can use this one :  \n",
    "Gmail adress : [mounierseb93@gmail.com]  \n",
    "Code : [mounse$15]"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>To authorize access needed by Earth Engine, open the following\n",
       "        URL in a web browser and follow the instructions:</p>\n",
       "        <p><a href=https://accounts.google.com/o/oauth2/auth?client_id=517222506229-vsmmajv00ul0bs7p89v5m89qs8eb9359.apps.googleusercontent.com&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fearthengine+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdevstorage.full_control&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&response_type=code&code_challenge=CQEI37Knd3BGyQweGYC_v749VSjvKUbv9RVtMcKXjDg&code_challenge_method=S256>https://accounts.google.com/o/oauth2/auth?client_id=517222506229-vsmmajv00ul0bs7p89v5m89qs8eb9359.apps.googleusercontent.com&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fearthengine+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdevstorage.full_control&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&response_type=code&code_challenge=CQEI37Knd3BGyQweGYC_v749VSjvKUbv9RVtMcKXjDg&code_challenge_method=S256</a></p>\n",
       "        <p>The authorization workflow will generate a code, which you\n",
       "        should paste in the box below</p>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter verification code: 4/1AY0e-g7vT-onSm_sAAoOl1tekiw3amNXLvQ9bgQKlP2jHryH_GfodIDOzws\n",
      "\n",
      "Successfully saved authorization token.\n"
     ]
    }
   ],
   "source": [
    "import ee\n",
    "ee.Authenticate()\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "from tensorflow.keras import backend as K\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "gpu = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpu[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_construction import TFDatasetConstruction\n",
    "from dataset_loader import TFDatasetProcessing\n",
    "from unet import DLModel, ModelEvaluation\n",
    "from losses_and_metrics import Loss, Metric, get_class_weights\n",
    "from learning_rate import  LRFinder, CyclicLR, step_decay_schedule\n",
    "from inference import Inference, download_kml, download_tif, download_kml_from_tif\n",
    "from utils import predict_pipes, color_pipes, get_statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify inputs (Landsat bands) to the model and the response variable.\n",
    "LANDSAT  = ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B10', 'B11']\n",
    "SENTINEL = ['VV','VH','VV_1','VH_1']\n",
    "BANDS    = LANDSAT + SENTINEL\n",
    "RESPONSE = 'landcover'\n",
    "FEATURES = BANDS+[RESPONSE]\n",
    "\n",
    "# Specify the size and shape of patches expected by the model.\n",
    "KERNEL_SIZE   = 128 #@param {type:\"integer\"}\n",
    "KERNEL_SHAPE  = [KERNEL_SIZE, KERNEL_SIZE]\n",
    "COLUMNS       = [tf.io.FixedLenFeature(shape=KERNEL_SHAPE, dtype=tf.float32) for k in FEATURES]\n",
    "FEATURES_DICT = dict(zip(FEATURES, COLUMNS))\n",
    "NUM_FEATURES  = len(BANDS)\n",
    "NUM_CLASSES   = 4 #@param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Dataset Loading and Processing\n",
    "If you don't have access to the training dataset, download it from the Google Drive in this address and password (and make sur to add it the right folder with the same name as in the drive) :    \n",
    "Gmail adress : [mounierseb93@gmail.com]  \n",
    "Code : [mounse$15]  \n",
    "If it's not there for some reason, or if you want to construct your own dataset, go to tuto_dataset_construction.ipynb and run it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify training parameters\n",
    "BATCH_SIZE = 4 #@param {type:\"integer\"}\n",
    "TRAIN_SIZE = 5000 #@param {type:\"integer\"}\n",
    "EVAL_SIZE  = 3000 #@param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process training and evaluation tf.Datasets\n",
    "tfdataloader = TFDatasetProcessing(FEATURES_DICT,FEATURES,BANDS,NUM_FEATURES)\n",
    "training     = tfdataloader.get_training_dataset()\n",
    "evaluation   = tfdataloader.get_eval_dataset()\n",
    "NUM_FEATURES = tfdataloader.num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iter(evaluation.take(1)).next())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify training parameters\n",
    "MODEL_NAME = 'unet' #@param {type:\"string\"}\n",
    "LOSS_STR = 'weighted_scc' #@param [\"scc\", \"weighted_scc\",\"dice\",\"jaccard\"]\n",
    "OPTIMIZER = 'sgd' #@param [\"sgd\",\"adam\",\"rmsprop\"]\n",
    "HISTORY_FILE = 'results/'+MODEL_NAME+'.log'\n",
    "CHECKPOINT_FILE = 'models/'+MODEL_NAME\n",
    "\n",
    "#Callbacks\n",
    "MONITOR_EARLY_STOP = 'loss' #@param [\"loss\",\"val_loss\"]\n",
    "MONITOR_CHECKPOINT = 'loss' #@param [\"loss\",\"val_loss\"]\n",
    "csv_logger = tf.keras.callbacks.CSVLogger(HISTORY_FILE, separator=',', append=False)\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor=MONITOR_EARLY_STOP, mode='min', verbose=2,min_delta=0.001,patience=4)\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(CHECKPOINT_FILE, monitor=MONITOR_CHECKPOINT, verbose=0, save_best_only=True,save_weights_only=False, mode='min',                    save_freq=int(TRAIN_SIZE / BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.generic_utils import get_custom_objects\n",
    "class_weights = get_class_weights(training,TRAIN_SIZE,KERNEL_SIZE,NUM_CLASSES)\n",
    "losses        = Loss(class_weights,NUM_CLASSES)\n",
    "metrics       = Metric(NUM_CLASSES)\n",
    "\n",
    "LOSS = losses.get_loss(LOSS_STR)\n",
    "get_custom_objects().update({\"loss\": LOSS})\n",
    "\n",
    "METRICS = ['sparse_categorical_accuracy',metrics.f1_score]\n",
    "get_custom_objects().update({\"f1_score\": metrics.f1_score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelconstructor = DLModel(training,NUM_FEATURES,NUM_CLASSES,BATCH_SIZE,OPTIMIZER,LOSS,METRICS,CHECKPOINT_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal learning rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_LR    = 1e-2 #@param {type:\"number\"} \n",
    "MAX_LR    = 1e-1 #@param {type:\"number\"} \n",
    "EPOCHS    = 2 #@param {type:\"integer\"}\n",
    "lr_finder = LRFinder(min_lr=MIN_LR, \n",
    "                      max_lr=MAX_LR, \n",
    "                      steps_per_epoch=int(TRAIN_SIZE / BATCH_SIZE), \n",
    "                      epochs=EPOCHS)\n",
    "\n",
    "FROM_CHECKPOINT = True #@param{type:\"boolean\"}\n",
    "\n",
    "m = modelconstructor.init_model(from_checkpoint=FROM_CHECKPOINT)\n",
    "m.fit(\n",
    "    x=training, \n",
    "    epochs=EPOCHS, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    steps_per_epoch=int(TRAIN_SIZE / BATCH_SIZE), \n",
    "    callbacks=[lr_finder,checkpoint])\n",
    "lr_finder.plot_loss()\n",
    "\n",
    "ind_max = np.argmax(lr_finder.history['sparse_categorical_accuracy'])\n",
    "print('The lr corresponding to the maximal metric = ',lr_finder.history['lr'][ind_max])\n",
    "ind_min = np.argmin(lr_finder.history['loss'])\n",
    "print('The lr corresponding to the minimal loss = ',lr_finder.history['lr'][ind_min])\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Staircase learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the initial learning rate as the optimal learning rate found just above\n",
    "INITIAL_LR   = 0.08 #@param {type:\"number\"}\n",
    "DECAY_FACTOR = 0.9 #@param {type:\"number\"}\n",
    "STEP_SIZE    = 5 #@param {type:\"integer\"}\n",
    "lr_step      = step_decay_schedule(initial_lr=INITIAL_LR, decay_factor=DECAY_FACTOR, step_size=STEP_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cyclical learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_LR   =  1e-8 #@param {type:\"number\"}\n",
    "MAX_LR    =  5e-8 #@param {type:\"number\"}\n",
    "STEP_SIZE =  1 #@param {type:\"number\"}\n",
    "MODE      = 'triangular2' #@param [\"triangular\",\"triangular2\",\"exp_range\"]\n",
    "clr       = CyclicLR(base_lr=BASE_LR, max_lr=MAX_LR,step_size=STEP_SIZE*int(TRAIN_SIZE / BATCH_SIZE), mode=MODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CALLBACKS = [csv_logger,checkpoint]\n",
    "\n",
    "lr_sched = 'exponential' #@param [\"exponential\",\"cyclical\",\"constant\"]\n",
    "\n",
    "if lr_sched =='constant' :\n",
    "  K.set_value(self.model.optimizer.lr, ind_max)\n",
    "elif lr_sched == 'exponential' :\n",
    "  CALLBACKS.append(lr_step)\n",
    "elif lr_sched == 'cyclical' :\n",
    "  CALLBACKS.append(clr)\n",
    "else : \n",
    "  raise NotImplementedError('Unrecognised schedule')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Training and Evaluation of Model\n",
    "\n",
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FROM_CHECKPOINT = True #@param{type:\"boolean\"}\n",
    "#m = modelconstructor.init_model(from_checkpoint=FROM_CHECKPOINT)\n",
    "\n",
    "EPOCHS =  100 #@param {type:\"integer\"}\n",
    "\n",
    "m.fit(\n",
    "    x=training, \n",
    "    epochs=EPOCHS, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    steps_per_epoch=int(TRAIN_SIZE / BATCH_SIZE), \n",
    "    #validation_data=evaluation,\n",
    "    #validation_steps=EVAL_SIZE,s\n",
    "    callbacks=CALLBACKS)\n",
    "\n",
    "m.optimizer.lr\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_NAMES  = [0,1,2,3]\n",
    "TARGET_NAMES = ['field','forest','urbain','water']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = ModelEvaluation(MODEL_NAME,EVAL_SIZE,TARGET_NAMES,LABEL_NAMES)\n",
    "evaluator.evaluate(m,evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have three options to how you create your test image :  \n",
    ">**1)** Export an image from a square window of a given center [[Lon,Lat]] and [radius] (in meters)   \n",
    ">**2)** Export an image from a window given a bounding box [[Lon1, Lat1, Lon2, Lat2]]  \n",
    ">**3)** Export the whole area of a network* (for the brave who want to use Earth Engine's Editor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*CAREFUL* ! if you want to **export the whole area of a network** (the third option) : \n",
    "\n",
    "If your network is not already uploaded to your **Google Earth Editor Assets** (I have already added sieccao, saur (zone1) and brioude), either provide a bounding box covering the whole area of the network (follow the second option) OR follow these steps :\n",
    ">\n",
    ">- If your file is not a **shp** file, for example a **kml**, convert it to \".shp\" using QGIS :\n",
    ">    * Drag your kml to the QGIS window.\n",
    ">    * Right-click on your layer and choose \"Export\" then \"Export Feature As\"\n",
    ">    * Set \"Format\" as \"ESRI Shapefile\" and the \"CRS\" as \"EPSG:3857 / Pseudo-Mercator\"\n",
    ">    * Fill \"File name\" to the name of the file. Careful, you should provide the directory : example \"C:\\....pipe.shp\"\n",
    ">    * Click on \"OK\" and wait, this could take a moment. Now you have created several files, please keep them all.\n",
    ">- Upload your files to your Google Earth Engine Editor :\n",
    ">    * Go to https://code.earthengine.google.com/\n",
    ">    * Click on \"Assets\", then \"NEW\", then below \"Table Upload\", click on \"Shape files\". Select all the files you just >created with QGIS.\n",
    ">    * Set Assetid to the name of your network ie \"brioude\"\n",
    ">    * Click on \"UPLOAD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify inference parameters\n",
    "start_date = \"2020-01-01\"\n",
    "end_date   = \"2020-12-31\"\n",
    "image_name   = 'test' #Name your image as you want'\n",
    "\n",
    "#FILL ONLY ONE OF THE FOLLOWING :\n",
    "\n",
    "# 1) if you want to export an image from a square window of a given center and radius (meters) :\n",
    "lon    = None #@param {type;'number'}\n",
    "lat    = None #@param {type:'number'}\n",
    "point  = [lon,lat]\n",
    "radius = None #@param {type:'number'} (in meters)\n",
    "\n",
    "# 2) if you want to export an image from a window given a bounding box\n",
    "minLng    = None #@param {type:'number'}\n",
    "minLat    = None #@param {type:'number'}\n",
    "maxLng    = None #@param {type:'number'}\n",
    "maxLat    = None #@param {type:'number'}\n",
    "rectangle = [minLng, minLat, maxLng, maxLat]\n",
    "\n",
    "# 3) if you want to export the whole area of a network\n",
    "# can be 'brioude','sieccao','saur' or the name of your network you just created following the tutorial above\n",
    "network_name = None #@param {type:'string'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct inference dataset\n",
    "tfdataconstructor = TFDatasetConstruction(LANDSAT,SENTINEL,RESPONSE,KERNEL_SIZE)\n",
    "corners = tfdataconstructor.test_dataset_construction(start_date,end_date,image_name,patch_size=KERNEL_SHAPE,radius=radius,point=point,length=length,rectangle=rectangle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and predict inference dataset and write predictions\n",
    "tfdataloader = TFDatasetProcessing(FEATURES_DICT,FEATURES,BANDS,NUM_FEATURES)\n",
    "testdataset  = tfdataloader.get_inference_dataset(image_name)\n",
    "NUM_FEATURES = tfdataloader.num_features\n",
    "\n",
    "inference = Inference(NUM_CLASSES,MODEL_NAME)\n",
    "predictions = inference.doDLPrediction(testdataset,image_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) Visualization\n",
    "On **GEEMap** OR on **Google Earth Pro**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note : replace **filename** by the name of your inference image in the tutorial below* .\n",
    "  \n",
    "You can visualize predictions directly on GEEMap OR on Google Earth Pro by creating a kml file.\n",
    "*But*  you need to georeference your predictions by adding to Google Earth Editor the files you just created : **filename.tfrecord** and **filename-mixer.json**. To do so, follow the following steps :\n",
    "- Go to https://code.earthengine.google.com/\n",
    "- Click on \"Assets\", then \"NEW\", then below \"Image Upload\", click on \"GeoTIFF\".\n",
    "- In \"Sources files\" select the **filename.TFRecord** file folder 'tfrecords' under 'predictions' (ie data/predictions/tfrecords/filename.TFRecord)\n",
    "- Add to \"Sources files\" the **filename-mixer.json** file in your folder 'inference' (ie data/inference/filename-mixer.json)\n",
    "- Set \"AssetId\" to \"filename_pred\"\n",
    "- Click on \"UPLOAD\"\n",
    "- On the right corner of your screen, click on \"Tasks\". Check the status of your export.\n",
    "- If there's an error \"cannot read mixer file\", retry the steps above by putting the mixer file before the tfrecord file and vise-versa several times until you succeed, the system bugs sometimes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise on GMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geemap.folium as gmap\n",
    "Map = gmap.Map()\n",
    "predictions = ee.Image('users/lealm/'+image_name+'_pred')\n",
    "Map.addLayer(predictions,{'min':0,'max':3,'palette':['lime','darkgreen','yellow','blue']},'predictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export a tif and kml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image export completed\n",
      "Download image test_image_pred from drive (directory data/predictions) if you work on your local computer\n"
     ]
    }
   ],
   "source": [
    "ASSETID = image_name + '_pred'\n",
    "# Since Earth Engine allows export only in tif format, we export the tif first and then we convert it to kml\n",
    "download_tif(ASSETID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kml saved at \"data/predictions/kml\"\n"
     ]
    }
   ],
   "source": [
    "download_kml_from_tif('data/predictions/'+ASSETID+'.tif')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}