{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Pipeline\n",
    "<img src=\"tuto_images/9.PNG?modified=02022021200000\" width=\"800\" >  \n",
    "\n",
    "#### You are in the deep learning model training step  \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=red> Welcome to this deep learning model training tutorial ! <font>\n",
    "            \n",
    "## There are 7 different steps :   \n",
    ">- 1) Import libraries and define parameters.  \n",
    ">    *Parameters followed by **#@param** are variables, you can change them.*\n",
    ">- 2) Construct your dataset. (No need if you already have one)\n",
    ">- 3) Load and process your dataset.\n",
    ">- 4) Set/find the loss, metrics and optimal hyper-parameters.\n",
    ">- 5) Train and evaluate your model.  \n",
    ">    *If you want to load a pretrained model, you can skip these steps and go to the next section.*\n",
    ">- 6) Predict your image.\n",
    ">- 7) Visualise your predictions\n",
    "\n",
    "### Table of Contents\n",
    "* [I. Libraries and Variables](#I.-Libraries-and-Variables)\n",
    "* [II. Dataset Construction](#II.-Dataset-Construction)\n",
    "* [III. Dataset Loading and Processing](#III.-Dataset-Loading-and-Processing)\n",
    "* [IV. Hyperparameters Tuning](#IV.-Hyperparameters-Tuning)\n",
    "* [V. Model Training and Evaluation](#V.-Model-Training-and-Evaluation)\n",
    "* [VI. Inference](#VI.-Inference)\n",
    "* [VII. Visualization](#VII.-Visualization)\n",
    "* [VIII. Earth Engine Editor Tutorials](#VIII.-Tutorials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Libraries and Variables\n",
    "### First create a new virtual environment (you will have problems with your package versions otherwise)  \n",
    "### Then install all requirements by running the following :\n",
    "\n",
    "*Careful* if you have multiple python versions :\n",
    "- If you are on your default python, run : **!pip** install -r requirements.txt\n",
    "- If you are on another python version, add your version number to pip. Example, if you are working on python3.8 run **!pip3.8** install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which python you use for your default pipe\n",
    "!pip --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the version of pip if needed\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.keras.backend.clear_session()\n",
    "gpu = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpu[0], True)\n",
    "from tensorflow.keras import backend as K\n",
    "import numpy as np \n",
    "import os\n",
    "\n",
    "from dataset_construction import TFDatasetConstruction\n",
    "from dataset_loader import TFDatasetProcessing\n",
    "from unet import DLModel, ModelEvaluation\n",
    "from losses_and_metrics import Loss, Metric, get_class_weights\n",
    "from learning_rate import  LRFinder, CyclicLR, step_decay_schedule\n",
    "from inference import download_tif, download_kml_from_tif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import, authenticate and initialize the Earth Engine library.  \n",
    "If you have a gmail account and already have access to Earth Engine, do so with yours, if not, you can use this one. It was created for the purpose of this project:  \n",
    "Gmail adress : `mounierseb93@gmail.com`    \n",
    "Code : `mounse$15`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "ee.Authenticate()\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create all folders you will need\n",
    "After running the following, your directory shoud be as following :\n",
    "Check if the folders are in your directory.\n",
    "- **Main folder**\n",
    "    * models\n",
    "    * results\n",
    "    * data\n",
    "        * train\n",
    "        * eval\n",
    "        * predictions\n",
    "            * colored_pipes\n",
    "            * kml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_folders()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANDSAT  = ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B10', 'B11']\n",
    "SENTINEL = ['VV','VH','VV_1','VH_1']\n",
    "BANDS    = LANDSAT + SENTINEL\n",
    "RESPONSE = 'landcover'\n",
    "FEATURES = BANDS+[RESPONSE]\n",
    "\n",
    "KERNEL_SIZE   = 128 #@param {type:\"integer\"}\n",
    "KERNEL_SHAPE  = [KERNEL_SIZE, KERNEL_SIZE]\n",
    "COLUMNS       = [tf.io.FixedLenFeature(shape=KERNEL_SHAPE, dtype=tf.float32) for k in FEATURES]\n",
    "FEATURES_DICT = dict(zip(FEATURES, COLUMNS))\n",
    "NUM_FEATURES  = len(BANDS)\n",
    "NUM_CLASSES   = 4 #@param {type:\"integer\"}\n",
    "\n",
    "LABEL_NAMES  = [0,1,2,3]\n",
    "TARGET_NAMES = ['field','forest','urban','water']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify training parameters\n",
    "BATCH_SIZE = 4 #@param {type:\"integer\"}\n",
    "TRAIN_SIZE = 5000 #@param {type:\"integer\"}\n",
    "EVAL_SIZE  = 3000 #@param {type:\"integer\"}\n",
    "\n",
    "# Specify training parameters\n",
    "MODEL_NAME = 'unet' #@param {type:\"string\"}\n",
    "LOSS_STR = 'weighted_scc' #@param [\"scc\", \"weighted_scc\",\"dice\",\"jaccard\"]\n",
    "OPTIMIZER = 'sgd' #@param [\"sgd\",\"adam\",\"rmsprop\"]\n",
    "HISTORY_FILE = 'results/'+MODEL_NAME+'.log'\n",
    "CHECKPOINT_FILE = 'models/'+MODEL_NAME\n",
    "\n",
    "# Callbacks\n",
    "MONITOR_EARLY_STOP = 'loss' #@param [\"loss\",\"val_loss\"]\n",
    "MONITOR_CHECKPOINT = 'loss' #@param [\"loss\",\"val_loss\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Dataset Construction  \n",
    "a) First connect to Google Drive using this address and password :  \n",
    "Gmail adress : [mounierseb93@gmail.com]  \n",
    "Code : [mounse$15]\n",
    "\n",
    "**Every time your run a code, if you receive a message like this : \"Please download file from Drive from folder ...\", go to the Google Drive and to the folder mentioned, and download the file in the same folder on your computer.**\n",
    "\n",
    "b) Run the following only if you haven't already access to the training dataset (.tfrecord.gz in folders 'train' and 'eval') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export training and evaluation tfrecords\n",
    "tfdataconstructor = TFDatasetConstruction(LANDSAT,SENTINEL,RESPONSE,KERNEL_SIZE)\n",
    "tfdataconstructor.dataset_construction(\"2017-01-01\",\"2017-12-31\") #the date should not change since the label dataset is from 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Dataset Loading and Processing\n",
    "If you don't have access to the training dataset, download it from the Google Drive in this address and password (and make sur to add it the right folder with the same name as in the drive) :    \n",
    "Gmail adress : [mounierseb93@gmail.com]  \n",
    "Code : [mounse$15]  \n",
    "If it's not there for some reason, or if you want to construct your own dataset, go to tuto_dataset_construction.ipynb and run it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process training and evaluation tf.Datasets\n",
    "tfdataloader = TFDatasetProcessing(FEATURES_DICT,FEATURES,BANDS,NUM_FEATURES)\n",
    "training     = tfdataloader.get_training_dataset()\n",
    "evaluation   = tfdataloader.get_eval_dataset()\n",
    "NUM_FEATURES = tfdataloader.num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iter(evaluation.take(1)).next())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Hyperparameters Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [1. Optimal Learning Rate](#1.-Optimal-Learning-Rate)\n",
    "Find the best learning rate to start with, then choose a scheduler : constant, staircase or cyclical learning rate\n",
    "* [2. Scheduler : Staircase Learning Rate](#2.-Scheduler-:-Staircase-Learning-Rate)\n",
    "* [3. Scheduler : Cyclical Learning Rate](#3.-Scheduler-:-Cyclical-Learning-Rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Optimal Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_LR    = 1e-2 #@param {type:\"number\"} \n",
    "MAX_LR    = 1e-1 #@param {type:\"number\"} \n",
    "EPOCHS    = 2 #@param {type:\"integer\"}\n",
    "lr_finder = LRFinder(min_lr=MIN_LR, \n",
    "                      max_lr=MAX_LR, \n",
    "                      steps_per_epoch=int(TRAIN_SIZE / BATCH_SIZE), \n",
    "                      epochs=EPOCHS)\n",
    "\n",
    "FROM_CHECKPOINT = True #@param{type:\"boolean\"}\n",
    "\n",
    "m = modelconstructor.init_model(from_checkpoint=FROM_CHECKPOINT)\n",
    "m.fit(\n",
    "    x=training, \n",
    "    epochs=EPOCHS, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    steps_per_epoch=int(TRAIN_SIZE / BATCH_SIZE), \n",
    "    callbacks=[lr_finder,checkpoint])\n",
    "lr_finder.plot_loss()\n",
    "\n",
    "ind_max = np.argmax(lr_finder.history['sparse_categorical_accuracy'])\n",
    "print('The lr corresponding to the maximal metric = ',lr_finder.history['lr'][ind_max])\n",
    "ind_min = np.argmin(lr_finder.history['loss'])\n",
    "print('The lr corresponding to the minimal loss = ',lr_finder.history['lr'][ind_min])\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Scheduler : Staircase Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the initial learning rate as the optimal learning rate found just above\n",
    "INITIAL_LR   = 0.08 #@param {type:\"number\"}\n",
    "DECAY_FACTOR = 0.9 #@param {type:\"number\"}\n",
    "STEP_SIZE    = 5 #@param {type:\"integer\"}\n",
    "lr_step      = step_decay_schedule(initial_lr=INITIAL_LR, decay_factor=DECAY_FACTOR, step_size=STEP_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Scheduler : Cyclical Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_LR   =  1e-8 #@param {type:\"number\"}\n",
    "MAX_LR    =  5e-8 #@param {type:\"number\"}\n",
    "STEP_SIZE =  1 #@param {type:\"number\"}\n",
    "MODE      = 'triangular2' #@param [\"triangular\",\"triangular2\",\"exp_range\"]\n",
    "clr       = CyclicLR(base_lr=BASE_LR, max_lr=MAX_LR,step_size=STEP_SIZE*int(TRAIN_SIZE / BATCH_SIZE), mode=MODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_sched = 'staircase' #@param [\"staircase\",\"cyclical\",\"constant\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "csv_logger = tf.keras.callbacks.CSVLogger(HISTORY_FILE, separator=',', append=False)\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor=MONITOR_EARLY_STOP, mode='min', verbose=2,min_delta=0.001,patience=4)\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(CHECKPOINT_FILE, monitor=MONITOR_CHECKPOINT, verbose=0, save_best_only=True,save_weights_only=False, mode='min',save_freq=int(TRAIN_SIZE / BATCH_SIZE))\n",
    "CALLBACKS = [csv_logger,checkpoint]\n",
    "\n",
    "if lr_sched =='constant' :\n",
    "  K.set_value(self.model.optimizer.lr, ind_max)\n",
    "elif lr_sched == 'staircase' :\n",
    "  CALLBACKS.append(lr_step)\n",
    "elif lr_sched == 'cyclical' :\n",
    "  CALLBACKS.append(clr)\n",
    "else : \n",
    "  raise NotImplementedError('Unrecognised schedule')\n",
    "\n",
    "# Losses\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "class_weights = get_class_weights(training,TRAIN_SIZE,KERNEL_SIZE,NUM_CLASSES)\n",
    "losses        = Loss(class_weights,NUM_CLASSES)\n",
    "LOSS          = losses.get_loss(LOSS_STR)\n",
    "get_custom_objects().update({\"loss\": LOSS})\n",
    "\n",
    "# Metrics\n",
    "metrics = Metric(NUM_CLASSES)\n",
    "METRICS = ['sparse_categorical_accuracy',metrics.f1_score]\n",
    "get_custom_objects().update({\"f1_score\": metrics.f1_score})\n",
    "\n",
    "# Model\n",
    "modelconstructor = DLModel(training,NUM_FEATURES,NUM_CLASSES,BATCH_SIZE,OPTIMIZER,LOSS,METRICS,CHECKPOINT_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V. Model Training and Evaluation\n",
    "\n",
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FROM_CHECKPOINT = True #@param{type:\"boolean\"}\n",
    "#m = modelconstructor.init_model(from_checkpoint=FROM_CHECKPOINT)\n",
    "\n",
    "EPOCHS =  100 #@param {type:\"integer\"}\n",
    "\n",
    "m.fit(\n",
    "    x=training, \n",
    "    epochs=EPOCHS, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    steps_per_epoch=int(TRAIN_SIZE / BATCH_SIZE), \n",
    "    #validation_data=evaluation,\n",
    "    #validation_steps=EVAL_SIZE,s\n",
    "    callbacks=CALLBACKS)\n",
    "\n",
    "m.optimizer.lr\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = ModelEvaluation(MODEL_NAME,EVAL_SIZE,TARGET_NAMES,LABEL_NAMES)\n",
    "evaluator.evaluate(m,evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VI. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how inference works, \n",
    "* You specify the input location,\n",
    "* The code will extract the test image in that location\n",
    "* The pretrained model will process the input image and produce a mask image.\n",
    "    \n",
    "<img src=\"tuto_images/8bis.PNG?modified=08022021170000\" width=\"800\">  \n",
    "\n",
    "You have two options to how you create your test image :  \n",
    ">**1)** Export an image from a square window of a given center ```[Lon,Lat]``` and ```radius``` (in meters)   \n",
    ">**2)** Export an image from a window given a bounding box ```[Lon1, Lat1, Lon2, Lat2]``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify inference parameters\n",
    "start_date = \"2020-01-01\"\n",
    "end_date   = \"2020-12-31\"\n",
    "image_name   = 'test' #Name your image as you want'\n",
    "\n",
    "#Do not change the following\n",
    "lon    = None \n",
    "lat    = None\n",
    "point  = [lon,lat]\n",
    "radius = None\n",
    "minLng = None\n",
    "minLat = None \n",
    "maxLng = None \n",
    "maxLat = None\n",
    "rectangle = [minLng, minLat, maxLng, maxLat]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have two options to how you create your test image :  \n",
    "* [1. Export an image from a square window of a given center `[Lon,Lat]` and `radius` (in meters)](#1.-Export-an-image-from-a-square-window-of-given-center-and-radius)\n",
    "* [2. Export an image from a window given a bounding box `[Lon1, Lat1, Lon2, Lat2]`](#2.-Export-an-image-from-a-window-given-a-bounding-box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill only one of the following :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Export an image from a square window of given center and radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lon    = None #@param {type;'number'}\n",
    "lat    = None #@param {type:'number'}\n",
    "point  = [lon,lat]\n",
    "radius = None #@param {type:'number'} #(in meters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Export an image from a window given a bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minLng    = None #@param {type:'number'}\n",
    "minLat    = None #@param {type:'number'}\n",
    "maxLng    = None #@param {type:'number'}\n",
    "maxLat    = None #@param {type:'number'}\n",
    "rectangle = [minLng, minLat, maxLng, maxLat]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct inference dataset\n",
    "tfdataconstructor = TFDatasetConstruction(LANDSAT,SENTINEL,RESPONSE,KERNEL_SIZE)\n",
    "corners = tfdataconstructor.test_dataset_construction(start_date,end_date,image_name,patch_size=KERNEL_SHAPE,point=point,radius=radius,rectangle=rectangle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and predict inference dataset and write predictions\n",
    "tfdataloader = TFDatasetProcessing(FEATURES_DICT,FEATURES,BANDS,NUM_FEATURES)\n",
    "testdataset  = tfdataloader.get_inference_dataset(image_name)\n",
    "NUM_FEATURES = tfdataloader.num_features\n",
    "\n",
    "inference = Inference(NUM_CLASSES,MODEL_NAME)\n",
    "predictions = inference.doDLPrediction(testdataset,image_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VII. Visualization\n",
    "On **GEEMap** OR on **Google Earth Pro**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note : replace **filename** by the name of your inference image in the tutorial below* .\n",
    "  \n",
    "You can visualize predictions directly on GEEMap OR on Google Earth Pro by creating a kml file.\n",
    "*But*  you need to georeference your predictions by adding to Google Earth Editor the files you just created. To do so, foll the tutorial [Add Image to Assets](#2.-Add-Image-to-Assets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Visualise on GMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geemap.folium as gmap\n",
    "Map = gmap.Map()\n",
    "predictions = ee.Image('users/lealm/'+image_name+'_pred')\n",
    "Map.addLayer(predictions,{'min':0,'max':3,'palette':['lime','darkgreen','yellow','blue']},'predictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Export a tif and kml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image export completed\n",
      "Download image test_image_pred from drive (directory data/predictions) if you work on your local computer\n"
     ]
    }
   ],
   "source": [
    "ASSETID = image_name + '_pred'\n",
    "# Since Earth Engine allows export only in tif format, we export the tif first and then we convert it to kml\n",
    "download_tif(ASSETID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kml saved at \"data/predictions/kml\"\n"
     ]
    }
   ],
   "source": [
    "download_kml_from_tif('data/predictions/'+ASSETID+'.tif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIII. Tutorials\n",
    "The Earth Engine (EE) Code Editor at https://code.earthengine.google.com is a web-based IDE for the Earth Engine JavaScript API. Code Editor features are designed to make developing complex geospatial workflows fast and easy. The Code Editor has the following elements :\n",
    "* JavaScript code editor\n",
    "* Map display for visualizing geospatial datasets\n",
    "* API reference documentation (Docs tab)\n",
    "* Git-based Script Manager (Scripts tab)\n",
    "* Console output (Console tab)\n",
    "* Task Manager (Tasks tab) to handle long-running queries\n",
    "* Interactive map query (Inspector tab)\n",
    "* Search of the data archive or saved scripts\n",
    "* Geometry drawing tools\n",
    "  \n",
    "<img src=\"tuto_images/earth-engine-code-editor.PNG\" width=500>\n",
    "\n",
    "Read https://developers.google.com/earth-engine/guides/playground for more information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Add Table to Assets\n",
    "\n",
    "- **1.** If your file is a \"kml\" file, convert it to a \".shp\" file by using this website https://mygeodata.cloud/converter/kml-to-shp OR by using **QGIS software** : \n",
    "  * **a.** Drag your kml to the QGIS window.\n",
    "  * **b.** Right-click on your layer and choose \"Export\" then \"Export Feature As\"   \n",
    "  <img src=\"tuto_images/3.PNG\" width=\"400\">  \n",
    "\n",
    "  * **c.** Set \"Format\" as \"ESRI Shapefile\" and the CRS as \"EPSG:3857 / Pseudo-Mercator\"  \n",
    "  * **d.** Fill \"File name\" to the name of the file. Careful, you should provide the directory : example C:\\....pipe.shp\n",
    "  * **e.** Click on \"OK\" and wait, this could take a moment. Now you have created several files, please keep them all.  \n",
    "  <img src=\"tuto_images/4.PNG\" width=\"400\"> \n",
    "    \n",
    "- **2.** Upload your files to your Google Earth Engine Editor :   \n",
    "  * **a.** Go to https://code.earthengine.google.com/\n",
    "  * **b.** Click on \"Assets\", then \"NEW\", then below \"Table Upload\", click on \"Shape files\". \n",
    "  <img src=\"tuto_images/5.PNG\" width=\"400\">\n",
    "  * **c.** Select all the files you just created with QGIS.  \n",
    "  * **d.** Name your Table, for example \"saur_zone2\"\n",
    "  * **e.** Click on \"UPLOAD\" \n",
    "  <img src=\"tuto_images/16.PNG\" width=\"400\">\n",
    "  - **f.** On the right corner of your screen, click on \"Tasks\". Check the status of your export.\n",
    "  <img src=\"tuto_images/17.PNG\" width=\"400\">\n",
    " \n",
    "Now you can access your table by typing : image = ee.FeatureCollection(assetid) with assetid = directory/network_name.\n",
    "You can find the assetid by clicking on the asset.  \n",
    "<img src=\"tuto_images/11.PNG?modified=02022021134100\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Add Image to Assets\n",
    "*replace `filename` by the name of your inference image in the tutorial below*\n",
    "\n",
    "- **1.** First, if not already done, add your network to Google Earth Editor's Assets following the tutorial in the previous section.\n",
    "- **2.** Go to https://code.earthengine.google.com/ .\n",
    "- **3.** Click on \"Assets\", then \"NEW\", then below \"Image Upload\", click on \"GeoTIFF\".  \n",
    "<img src=\"tuto_images/1.PNG\" width=\"400\">\n",
    "- **4.** In \"Sources files\" select the files `pred_filename.TFRecord` and `filename-mixer.json` in your folder 'inference'\n",
    "- **5.** Set \"AssetId\" to \"`filename`_pred\"\n",
    "- **6.** Click on \"UPLOAD\"  \n",
    "<img src=\"tuto_images/2.PNG\" width=\"400\">\n",
    "- **7.** On the right corner of your screen, click on \"Tasks\". Check the status of your export.  \n",
    "If there's an error \"cannot read mixer file\", retry the steps above by putting the mixer file before the tfrecord file and vise-versa several times until you succeed, the system bugs sometimes.\n",
    "<img src=\"tuto_images/6.PNG\" width=\"400\">\n",
    "\n",
    "Now you can access your image by typing : image = **ee.Image(assetid)** with assetid = directory/filename_pred.  \n",
    "You can find the assetid by clicking on the asset.  \n",
    "<img src=\"tuto_images/7.PNG?modified=02022021134700\" width=\"400\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
